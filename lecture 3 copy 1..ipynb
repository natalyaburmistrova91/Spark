{
  "metadata": {
    "name": "lecture 3 copy 1",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "sc.version"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col\n\ndf \u003d spark.read.json(sc.parallelize([\n  \"\"\"{\"properties\": { \"prop1\": \"foo\", \"prop2\": \"bar\", \"prop3\": true, \"prop4\": 1}}\"\"\"]\n))\n\ndf.select(col(\"properties.*\")).printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import expr\n\n\nhabrData \u003d spark.read\\\n    .option(\"header\", True)\\\n    .option(\"inferSchema\", True)\\\n    .csv(\"/user/admin/habr_data.csv\")\\\n    .withColumn(\"words_count\", expr(\"CAST(words_count as INTEGER)\"))\\\n    .cache()\n\nhabrData.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nz.show(habrData.where(\"words_count is null\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nz.show(habrData)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import regexp_replace, col\n\nhabrData\\\n.select(\"link\")\\\n.withColumn(\"company_id\", regexp_replace(col(\"link\"), \"(https://habr.com/ru/company/)|(/blog/[0-9]+/)|(https://habr.com/ru/post/[0-9]+/)\", \"\"))\\\n.show(50, False)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nhabrData \u003d spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/user/admin/habr_data.csv\").cache()\n\nhabrData.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nhabrData \u003d spark.read.option(\"header\", True).csv(\"/user/admin/habr_data.csv\").cache()\n \nfrom pyspark.sql.functions import udf, col, round\nfrom pyspark.sql.types import IntegerType, FloatType\n\ndef mult(i):\n    return i * 2\n    \nmultUdf \u003d udf(mult)\n\nhabrData\\\n.select(\"rating\")\\\n.limit(10)\\\n.withColumn(\"udfString\", multUdf(col(\"rating\")))\\\n.withColumn(\"udfInt\", multUdf(col(\"rating\").cast(IntegerType())))\\\n.withColumn(\"round\", round(col(\"udfInt\")) )\\\n.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nspark.createDataFrame(([1], [2], [3]), schema\u003d\"n INT\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nspark.createDataFrame(([1], [2], [3]), schema\u003d\"n INT\")\\\n.selectExpr(\"n + 1\")\\\n.explain()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import udf, col\n\ndef plusOne(i):\n    return i + 1\n    \nplusOneUdf \u003d udf(plusOne)\n\nspark.createDataFrame(([1], [2], [3]), schema\u003d\"n INT\")\\\n.select(plusOneUdf(col(\"n\")))\\\n.explain()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nspark.createDataFrame(([1], [2], [3]), schema\u003d\"n INT\")\\\n.selectExpr(\"n + 1 as plusOne\")\\\n.where(\"plusOne \u003d 2\")\\\n.explain()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nspark.createDataFrame(([1], [2], [3]), schema\u003d\"n INT\")\\\n.withColumn(\"plusOne\", plusOneUdf(col(\"n\")))\\\n.where(\"plusOne \u003d 2\")\\\n.explain()"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nhabrData \u003d spark.read.option(\"header\", True).csv(\"/user/admin/habr_data.csv\").cache()\n \nfrom pyspark.sql.functions import udf, col, when, expr\nfrom pyspark.sql.types import IntegerType\n\ndef mult(i):\n    return i * 2\n    \ndef mult_nullsafe(i):\n    if i is None:\n        return 0\n    else:\n        return i * 2\n    \nmultUdf \u003d udf(mult) \n# multUdf \u003d udf(mult_nullsafe)\n\n#.na.drop(\"all\")\\\nhabrData\\\n.select(\"rating\")\\\n.withColumn(\"rating\", col(\"rating\").cast(IntegerType()))\\\n.withColumn(\"udfInt\", multUdf(col(\"rating\"))   )\\\n.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import udf, col\n\n\nspark.udf.register(\"mult_nullsafe\", mult_nullsafe)\n\nhabrData\\\n.where(\"rating is not null\")\\\n.withColumn(\"rating\", col(\"rating\").cast(IntegerType()))\\\n.createOrReplaceTempView(\"habr_data\")\n\nspark.sql(\"select rating, mult_nullsafe(rating) plus_one from habr_data\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Integer type output\nfrom pyspark.sql.types import IntegerType\n\ndef square(i):\n    if i is None:\n        return 0\n    else:\n        return i * i\n    \nsquare_udf_int \u003d udf(lambda z: square(z), IntegerType())\n\nhabrData\\\n.select(\"rating\")\\\n.where(\"rating is not null\")\\\n.withColumn(\"rating\", col(\"rating\").cast(IntegerType()))\\\n.withColumn(\"square\", square_udf_int(col(\"rating\"))   )\\\n.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.types import *\n\n    \ndef square_list_float(x):\n    return [float(val)**2 for val in x]\n\n\nsquare_list_float_udf \u003d udf(lambda y: square_list_float(y), ArrayType(FloatType()))\n\n\ncSchema \u003d StructType([StructField(\"int_array\", ArrayType(IntegerType()))])\n\ndf \u003d spark.createDataFrame(\n    [[[1, 2]], [[3, 4, 5]], [[6, 7, 8, 9]]], schema\u003dcSchema\n)\n\ndf.printSchema()\ndf.show()\n\ndf\\\n.withColumn(\"square_list_float_udf\", square_list_float_udf(\"int_array\"))\\\n.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql import SparkSession\n\nspark \u003d SparkSession.builder.appName(\u0027SparkByExamples.com\u0027).getOrCreate()\n\nsimpleData \u003d [\\\n    (\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n  ]\n\ncolumns\u003d [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf \u003d spark.createDataFrame(data \u003d simpleData, schema \u003d columns)\ndf.printSchema()\ndf.show(truncate\u003dFalse)\n\nsimpleData2 \u003d [\\\n    (\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n  ]\ncolumns2\u003d [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n\ndf2 \u003d spark.createDataFrame(data \u003d simpleData2, schema \u003d columns2)\n\ndf2.printSchema()\ndf2.show(truncate\u003dFalse)\n\nunionDF \u003d df.union(df2)\nunionDF.show(truncate\u003dFalse)\ndisDF \u003d df.union(df2).distinct()\ndisDF.show(truncate\u003dFalse)\n\nunionAllDF \u003d df.unionAll(df2)\nunionAllDF.show(truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nsimpleData3 \u003d [\\\n    (\"Sales\",\"Jones\",\"NY\",90000,34,10000), \\\n    (\"Finance\",\"Maria\",\"CA\",90000,24,23000), \\\n    (\"Finance\",\"Jen\",\"NY\",79000,53,15000), \\\n  ]\ncolumns3\u003d [\"department\", \"employee_name\",\"state\",\"salary\",\"age\",\"bonus\"]\n\ndf3 \u003d spark.createDataFrame(data \u003d simpleData3, schema \u003d columns3)\n\nprint(\"df:\")\ndf.show()\nprint(\"df3:\")\ndf3.show()\n\nunionDF \u003d df.union(df3)\nunionDF.show(truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf.union(\n    df3.select(df.columns)\n).show(truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\narrayArrayData \u003d [\n  (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n]\n\ndf \u003d spark.createDataFrame(data\u003darrayArrayData, schema \u003d [\u0027name\u0027,\u0027subjects\u0027])\ndf.printSchema()\ndf.show(truncate\u003dFalse)\n\nfrom pyspark.sql.functions import explode\ndf2 \u003d df.select(df.name, explode(df.subjects).alias(\"exploded\"))\ndf2.show()\n\ndf2.select(df.name,explode(df2.exploded)).show(truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.schema.json()"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\narrayData \u003d [\n        (\u0027James\u0027,[\u0027Java\u0027,\u0027Scala\u0027],{\u0027hair\u0027:\u0027black\u0027,\u0027eye\u0027:\u0027brown\u0027}),\n        (\u0027Michael\u0027,[\u0027Spark\u0027,\u0027Java\u0027,None],{\u0027hair\u0027:\u0027brown\u0027,\u0027eye\u0027:None}),\n        (\u0027Robert\u0027,[\u0027CSharp\u0027,\u0027\u0027],{\u0027hair\u0027:\u0027red\u0027,\u0027eye\u0027:\u0027\u0027}),\n        (\u0027Washington\u0027,None,None),\n        (\u0027Jefferson\u0027,[\u00271\u0027,\u00272\u0027],{})\n        ]\n\ndf \u003d spark.createDataFrame(data\u003darrayData, schema \u003d [\u0027name\u0027, \u0027knownLanguages\u0027, \u0027properties\u0027])\ndf.printSchema()\ndf.show(truncate\u003dFalse)\n\nfrom pyspark.sql.functions import explode\ndf3 \u003d df.select(df.name,explode(df.properties))\ndf3.printSchema()\ndf3.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import explode_outer\n\"\"\" with array \"\"\"\ndf.select(df.name,explode_outer(df.knownLanguages)).show()\n\"\"\" with map \"\"\"\ndf.select(df.name,explode_outer(df.properties)).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#Window functions"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nsimpleData \u003d ((\"James\", \"Sales\", 3000), \\\n    (\"Michael\", \"Sales\", 4600),  \\\n    (\"Robert\", \"Sales\", 4100),   \\\n    (\"Maria\", \"Finance\", 3000),  \\\n    (\"James\", \"Sales\", 3000),    \\\n    (\"Scott\", \"Finance\", 3300),  \\\n    (\"Jen\", \"Finance\", 3900),    \\\n    (\"Jeff\", \"Marketing\", 3000), \\\n    (\"Kumar\", \"Marketing\", 2000),\\\n    (\"Saif\", \"Sales\", 4100) \\\n  )\n \ncolumns\u003d [\"employee_name\", \"department\", \"salary\"]\ndf \u003d spark.createDataFrame(data \u003d simpleData, schema \u003d columns)\ndf.printSchema()\ndf.show(truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, rank, dense_rank, percent_rank, ntile, cume_dist, round\nfrom pyspark.sql.types import FloatType\n\nwindowSpec \u003d Window.partitionBy(\"department\").orderBy(\"salary\")\n\ndf\\\n    .withColumn(\"row_number\", row_number().over(windowSpec)) \\\n    .withColumn(\"rank\", rank().over(windowSpec)) \\\n    .withColumn(\"dense_rank\", dense_rank().over(windowSpec)) \\\n    .withColumn(\"percent_rank\", percent_rank().over(windowSpec)) \\\n    .withColumn(\"ntile\", ntile(3).over(windowSpec)) \\\n    .withColumn(\"cume_dist\", round(cume_dist().over(windowSpec), 2 )) \\\n    .show(truncate\u003dFalse)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import lag, lead\n\ndf.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n      .show()"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nwindowSpecAgg  \u003d Window.partitionBy(\"department\")\n\nfrom pyspark.sql.functions import col,avg,sum,min,max,row_number\n\ndf.withColumn(\"row\",row_number().over(windowSpec)) \\\n.withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n.withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n.withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n.withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n.where(col(\"row\")\u003d\u003d1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import LongType\n\ndef usd2rub(usd_price):\n  return round(usd_price * 50, 2)\n  \nusd2rub_udf \u003d udf(usd2rub)\n\nretailData.where(\"UnitPrice is not null\")\\\n.withColumn(\"price_rub\", usd2rub_udf(\"UnitPrice\")).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "* по данным habr_data получить таблицу с названиями топ-3 статей (по rating) для каждого автора\n* по данным habr_data получить топ (по встречаемости) английских слов из заголовков. Возможное решение: 1) выделение слов с помощью регулярных выражений, 2) разделение на массивы слов 3) explode массивовов 4) группировка с подсчетом встречаемости"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    }
  ]
}