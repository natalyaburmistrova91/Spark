{
  "metadata": {
    "name": "lecture 2",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nschema \u003d \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY\u003cSTRING\u003e\"\n# Create our static data\ndata \u003d [\n    [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n    [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n    [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n    [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n    [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n    [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n]\n\n# Create a DataFrame using the schema defined above\nblogs_df \u003d spark.createDataFrame(data, schema)\n# Show the DataFrame; it should reflect our table above\nblogs_df.show()\n# Print the schema used by Spark to process the DataFrame\nprint(blogs_df.printSchema())"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import col, sum\n\nblogs_df\\\n.selectExpr(\"*\" , \"concat(first, \u0027 \u0027, last) full_name\")\\\n.agg(sum(col(\"Hits\")).alias(\"Hits\"))\\\n.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import expr\n\nblogs_df\\\n.where(\"hits \u003e 10000\")\\\n.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n\nsc.applicationId"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport os\nfrom pyspark.sql.types import *\n    \nSPARK_HOME \u003d os.getenv(\u0027SPARK_HOME\u0027)\n\n# Read data from json file\n# link for this people.json (https://github.com/apache/spark/blob/master/examples/src/main/resources/people.json)\n# Use hdfs path if you are using hdfs\ndf1 \u003d spark.read.json(\"file://\" + SPARK_HOME + \"/examples/src/main/resources/people.json\")\ndf1.printSchema()\ndf1.show()\n\n# Read data from csv file. You can customize it via spark.read.options. E.g. In the following example, we customize the sep and header\ndf2 \u003d spark.read.options(sep\u003d\";\", header\u003dTrue).csv(\"file://\"  + SPARK_HOME + \"/examples/src/main/resources/people.csv\")\ndf2.printSchema()\ndf2.show()\n\n# Specify schema for your csv file\nfrom pyspark.sql.types import StructType, StringType, IntegerType\n\nschema \u003d StructType().add(\"name\", StringType(), True) \\\n    .add(\"age\", IntegerType(), True) \\\n    .add(\"job\", StringType(), True)\n    \ndf3 \u003d spark.read\\\n    .options(sep\u003d\";\", header\u003dTrue) \\\n    .schema(schema) \\\n    .csv(\"file://\" + SPARK_HOME + \"/examples/src/main/resources/people.csv\") \n    \ndf3.printSchema()\ndf3.show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nspark.conf.get(\"spark.sql.files.maxPartitionBytes\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "spark.table(\"homework.bank\").show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\nWhat were all the different types of fire calls in 2018?\nWhat months within the year 2018 saw the highest number of fire calls?\nWhich neighborhood in San Francisco generated the most fire calls in 2018?\nWhich neighborhoods had the worst response times to fire calls in 2018?\nWhich week in the year in 2018 had the most fire calls?\nIs there a correlation between neighborhood, zip code, and number of fire calls?\nHow can we use Parquet files or SQL tables to store this data and read it back?\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfireDF \u003d spark.read.option(\"header\", True).csv(\"/user/admin/sf-fire-calls.csv\")\nfireDF.show(100, False)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import to_timestamp\nfrom pyspark.sql.functions import year, month, countDistinct\n\n\nfireDF\\\n.coalesce(1)\\\n.withColumn(\"CallDate\", to_timestamp(fireDF.CallDate, \u0027dd/MM/yyyy\u0027))\\\n.withColumn(\"CallYear\", year(\"CallDate\"))\\\n.where(\"CallYear \u003d \u00272018\u0027\") \\\n.select(\"CallType\")\\\n.distinct()\\\n.show(1000, False)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import to_timestamp\nfrom pyspark.sql.functions import year, month, countDistinct\n\nfireDF\\\n.withColumn(\"CallDate\", to_timestamp(fireDF.CallDate, \u0027dd/MM/yyyy\u0027))\\\n.withColumn(\"CallMonth\", month(\"CallDate\"))\\\n.withColumn(\"CallYear\", year(\"CallDate\"))\\\n.where(\"CallYear \u003d \u00272018\u0027\") \\\n.groupBy(\"CallMonth\")\\\n.agg(\n    countDistinct(\"callNumber\").alias(\"call_count\")\n    )\\\n.orderBy(\"call_count\", ascending\u003dFalse)\\\n.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n(fireDF\n.groupBy(\"Neighborhood\")\n.agg(countDistinct(\"callNumber\").alias(\"call_count\"))\n.orderBy(\"call_count\", ascending\u003dFalse)\n.show(1000, False))"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfireDF\\\n.where(\"lower(CallType) like \u0027%fire%\u0027\")\\\n.select(\"Neighborhood\", \"delay\", \"callType\")\\\n.orderBy(\"delay\", ascending\u003dFalse)\\\n.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import weekofyear, year\n\n\nfireDF\\\n.where(\"lower(CallType) like \u0027%fire%\u0027\")\\\n.withColumn(\"CallDate\", to_timestamp(fireDF.CallDate, \u0027dd/MM/yyyy\u0027))\\\n.withColumn(\"CallWeek\", weekofyear(\"CallDate\"))\\\n.withColumn(\"CallYear\", year(\"CallDate\"))\\\n.where(\"CallYear \u003d \u00272018\u0027\") \\\n.groupBy(\"CallYear\", \"CallWeek\")\\\n.agg(\n    countDistinct(\"callNumber\").alias(\"call_count\")\n    )\\\n.orderBy(\"call_count\", ascending\u003dFalse)\\\n.show()\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(sc.applicationId)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nprint(spark.conf.get(\"spark.sql.shuffle.partitions\"))\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", 20)\n\nprint(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "spark.table(\"homework.bank\")"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nprint(\"Hello \" + z.textbox(\"name\"))"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf1 \u003d spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]) \\\n            .toDF(\"id\", \"name\", \"age\", \"country\")\n\n# Create udf create python lambda\nfrom pyspark.sql.functions import udf\nudf1 \u003d udf(lambda e: e.upper())\ndf2 \u003d df1.select(udf1(df1[\"name\"]))\ndf2.show()\n\n# UDF could also be used in filter, in this case the return type must be Boolean\n# We can also use annotation to create udf\nfrom pyspark.sql.types import *\n@udf(returnType\u003dBooleanType())\ndef udf2(e):\n    if e \u003e\u003d 20:\n        return True;\n    else:\n        return False\n\ndf3 \u003d df1.filter(udf2(df1[\"age\"]))\ndf3.show()\n\n# UDF could also accept more than 1 argument.\nudf3 \u003d udf(lambda e1, e2: e1 + \"_\" + e2)\ndf4 \u003d df1.select(udf3(df1[\"name\"], df1[\"country\"]).alias(\"name_country\"))\ndf4.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf1 \u003d spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]) \\\n           .toDF(\"id\", \"name\", \"age\", \"country\")\n\n# You can call agg function after groupBy directly, such as count/min/max/avg/sum\ndf2 \u003d df1.groupBy(\"country\").count()\ndf2.show()\n\n# Pass a Map if you want to do multiple aggregation\ndf3 \u003d df1.groupBy(\"country\").agg({\"age\": \"avg\", \"id\": \"count\"})\ndf3.show()\n\nimport pyspark.sql.functions as F\n# Or you can pass a list of agg function\ndf4 \u003d df1.groupBy(\"country\").agg(F.avg(df1[\"age\"]).alias(\"avg_age\"), F.count(df1[\"id\"]).alias(\"count\"))\ndf4.show()\n\n# You can not pass Map if you want to do multiple aggregation on the same column as the key of Map should be unique. So in this case\n# you have to pass a list of agg functions\ndf5 \u003d df1.groupBy(\"country\").agg(F.avg(df1[\"age\"]).alias(\"avg_age\"), F.max(df1[\"age\"]).alias(\"max_age\"))\ndf5.show()\n\n\n\n\n\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf1 \u003d spark.createDataFrame([(1, \"andy\", 20, 1), (2, \"jeff\", 23, 2), (3, \"james\", 18, 3)]).toDF(\"id\", \"name\", \"age\", \"c_id\")\ndf1.show()\n\ndf2 \u003d spark.createDataFrame([(1, \"USA\"), (2, \"China\")]).toDF(\"c_id\", \"c_name\")\ndf2.show()\n\n# You can just specify the key name if join on the same key\ndf3 \u003d df1.join(df2, \"c_id\")\ndf3.show()\n\n# Or you can specify the join condition expclitly in case the key is different between tables\ndf4 \u003d df1.join(df2, df1[\"c_id\"] \u003d\u003d df2[\"c_id\"])\ndf4.show()\n\n# You can specify the join type afte the join condition, by default it is inner join\ndf5 \u003d df1.join(df2, df1[\"c_id\"] \u003d\u003d df2[\"c_id\"], \"left_outer\")\ndf5.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf1 \u003d spark.createDataFrame([(\"andy\", 20, 1, 1), (\"jeff\", 23, 1, 2), (\"james\", 12, 2, 2)]).toDF(\"name\", \"age\", \"key_1\", \"key_2\")\ndf1.show()\n\ndf2 \u003d spark.createDataFrame([(1, 1, \"USA\"), (2, 2, \"China\")]).toDF(\"key_1\", \"key_2\", \"country\")\ndf2.show()\n\n# Join on 2 fields: key_1, key_2\n\n# You can pass a list of field name if the join field names are the same in both tables\ndf3 \u003d df1.join(df2, [\"key_1\", \"key_2\"])\ndf3.show()\n\n# Or you can specify the join condition expclitly in case when the join fields name is differetnt in the two tables\ndf4 \u003d df1.join(df2, (df1[\"key_1\"] \u003d\u003d df2[\"key_1\"]) \u0026 (df1[\"key_2\"] \u003d\u003d df2[\"key_2\"]))\ndf4.show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf1 \u003d spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]) \\\n           .toDF(\"id\", \"name\", \"age\", \"country\")\n# call createOrReplaceTempView first if you want to query this DataFrame via sql\ndf1.createOrReplaceTempView(\"people\")\n# SparkSession.sql return DataFrame\ndf2 \u003d spark.sql(\"select name, age from people\")\ndf2.show()\n\n# You need to register udf if you want to use it in sql\nspark.udf.register(\"udf1\", lambda e : e.upper())\ndf3 \u003d spark.sql(\"select udf1(name), age from people\")\ndf3.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf \u003d spark.range(1 * 10000000).toDF(\"id\").withColumn(\"square\", col(\"id\") * col(\"id\"))\n\ndf.cache() # Cache the data\n\ndf.count() # Materialize the cache"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf.persist()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n// Generate some sample data for two data sets\n\nvar states \u003d scala.collection.mutable.Map[Int, String]()\nvar items \u003d scala.collection.mutable.Map[Int, String]()\nval rnd \u003d new scala.util.Random(42)\n\n// Initialize states and items purchased\n\nstates +\u003d (0 -\u003e \"AZ\", 1 -\u003e \"CO\", 2-\u003e \"CA\", 3-\u003e \"TX\", 4 -\u003e \"NY\", 5-\u003e \"MI\")\nitems +\u003d (0 -\u003e \"SKU-0\", 1 -\u003e \"SKU-1\", 2-\u003e \"SKU-2\", 3-\u003e \"SKU-3\", 4 -\u003e \"SKU-4\",\n5-\u003e \"SKU-5\")\n// Create DataFrames\nval usersDF \u003d (0 to 100000)\n    .map(id \u003d\u003e (id, s\"user_${id}\", s\"user_${id}@databricks.com\", states(rnd.nextInt(5))))\n    .toDF(\"uid\", \"login\", \"email\", \"user_state\")\n\n\nval ordersDF \u003d (0 to 100000)\n    .map(r \u003d\u003e (r, r, rnd.nextInt(10000), 10 * r* 0.2d, states(rnd.nextInt(5)), items(rnd.nextInt(5))))\n    .toDF(\"transaction_id\", \"quantity\", \"users_id\", \"amount\", \"state\", \"items\")\n    \n    \n// Do the join\nval usersOrdersDF \u003d ordersDF\n    .join(broadcast(usersDF), $\"users_id\" \u003d\u003d\u003d $\"uid\", \"left\")\n    .select(\"users_id\", \"transaction_id\")\n    \n// Show the joined results\nusersOrdersDF.show(false)"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "usersOrdersDF.explain()"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SaveMode\n\n// Save as managed tables by bucketing them in Parquet format\nusersDF.orderBy(asc(\"uid\"))\n    .write.format(\"parquet\")\n    .bucketBy(8, \"uid\")\n    .mode(\"overwrite\")\n    .saveAsTable(\"UsersTbl\")\n\nordersDF.orderBy(asc(\"users_id\"))\n    .write.format(\"parquet\")\n    .bucketBy(8, \"users_id\")\n    .mode(\"overwrite\")\n    .saveAsTable(\"OrdersTbl\")\n\n// Cache the tables\nspark.sql(\"CACHE TABLE UsersTbl\")\nspark.sql(\"CACHE TABLE OrdersTbl\")\n// Read them back in\n\nval usersBucketDF \u003d spark.table(\"UsersTbl\")\nval ordersBucketDF \u003d spark.table(\"OrdersTbl\")\n\n// Do the join and show the results\nval joinUsersOrdersBucketDF \u003d ordersBucketDF\n    .join(usersBucketDF, $\"users_id\" \u003d\u003d\u003d $\"uid\")\n\n\njoinUsersOrdersBucketDF.show(false)"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n// Generate some sample data for two data sets\n\nvar city \u003d scala.collection.mutable.Map[Int, String]()\nval rnd \u003d new scala.util.Random(42)\n\n// Initialize states and items purchased\n\ncity +\u003d (0 -\u003e \"Moscow\", 1 -\u003e \"Moscow\", 2-\u003e \"Moscow\", 3-\u003e \"Moscow\", 4 -\u003e \"Moscow\", 5 -\u003e \"Kazan\")\n\nfor(i \u003c- (1 to 100)) {\n    (0 to 100000)\n    .map(id \u003d\u003e (id, city(rnd.nextInt(6))))\n    .toDF(\"uid\", \"city\")\n    .withColumn(\"partition_id\", lit(i))\n    .write.mode(\"append\")\n    .saveAsTable(\"kotelnikov.uid_city\")\n}\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\nspark.table(\"kotelnikov.uid_city\")\n.groupBy(\"city\")\n.agg(\n    count(\"uid\").as(\"uid_count\")\n    )\n.show(100)"
    }
  ]
}