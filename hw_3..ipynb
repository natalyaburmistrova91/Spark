{
  "metadata": {
    "name": "hw_3",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nhabrData \u003d spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/user/admin/habr_data.csv\").cache()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "по данным habr_data получить таблицу с названиями топ-3 статей (по rating) для каждого автора"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, rank, dense_rank, percent_rank, ntile, cume_dist, round, col\nfrom pyspark.sql.types import FloatType\n\nwindowSpec \u003d Window.partitionBy(\"author_name\").orderBy(col(\"rating\").desc())\n\nhabrData.where(\"rating is not null\")\\\n    .withColumn(\"rating\", col(\"rating\").cast(FloatType()))\\\n    .withColumn(\"rank\", rank().over(windowSpec)) \\\n    .where(col(\"rank\") \u003c 4).select(\"author_name\",\"title\",\"rating\", \"rank\") \\\n    .show(50)\n    "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "по данным habr_data получить топ (по встречаемости) английских слов из заголовков. Возможное решение: 1) выделение слов с помощью регулярных выражений, 2) разделение на массивы слов 3) explode массивовов 4) группировка с подсчетом встречаемости\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import regexp_replace, udf, explode, split, col\nfrom pyspark.sql.types import StringType, ArrayType\n\nhd \u003d habrData.select(\u0027title\u0027).\\\n    withColumn(\u0027title_without_rus_words\u0027, regexp_replace(col(\u0027title\u0027), \u0027[^a-zA-Z\\s]\u0027, \u0027 \u0027)).\\\n    withColumn(\u0027title_without_rus_words_and_figures\u0027, regexp_replace(col(\u0027title_without_rus_words\u0027), \u0027\\s{2,}\u0027, \u0027 \u0027)).\\\n    withColumn(\u0027title_list\u0027, split(col(\u0027title_without_rus_words_and_figures\u0027), \u0027 \u0027)).\\\n    withColumn(\u0027words\u0027, explode(col(\u0027title_list\u0027))).\\\n    na.drop(\u0027all\u0027).\\\n    groupBy(\u0027words\u0027).\\\n    count().\\\n    orderBy(col(\u0027count\u0027).desc()).\\\n    where(col(\"words\") !\u003d \u0027\u0027).\\\n    select(\"words\",\"count\"). \\\n    show(50)\n\n"
    }
  ]
}